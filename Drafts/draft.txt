Literature Review: Auto-Encoding Variational Bayes

This paper proposes a way for training graphical models with unmeasurable posteriors and continuous latent variables. The reparametrization of latent variables are done so that a deterministic mapping can be devised. According to the authors, it enables the lower bound estimators to be different in comparision to the variational parameters. The previous work and existing models have been considered in brief and explanations have been provided. These include the Generative Stochastic Networks, Denoising Auto-Encoder theory and wake-sleep algorithm, which has a scope of improvements.

The authors have a proposed an application of auto encoding variational bayes where the priod is a Gaussian centered on zero and the mean of the distribution is determined by a neural network output. According to the authors, this addresses the parameter estimation problem in a graphical model. The proposed method has a fast training to get the parameters for data generation, approximation of the posterior for data representation and approximation of the marginal for the evalation of the model for other activities and models. The authors have discussed variious noise ditributions, but have not compared them in detail.

The quality of supporting evidence provided by the authors seems fair and extensive. Multiple experiments are run on both Frey-faces dataset and the Most dataset. The authors have compared the new lower bound with the wake-sleep algorithm, the new marginal likelihood with the likehood of wake-sleep algorithm. The visualization of 2D manifolds learned with the proposed solution and outlining of the samples got by sampling the dataset from the generative model is also shown by the authors in their study. However, they have not concentrated on the overfitting problem: when does it occur and how is it being handled by the new solution. The result of the expriments also reveal that the proposed model can improve wake-sleep algorithm.